{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "import csv\n",
    "import json\n",
    "\n",
    "# URL of the XML sitemap\n",
    "sitemap_url = 'https://www.trade-point.co.uk/static/sitemap.xml'\n",
    "\n",
    "# Download the XML file\n",
    "response = requests.get(sitemap_url)\n",
    "response.raise_for_status()  # Check if the request was successful\n",
    "\n",
    "# Parse the XML content\n",
    "tree = ET.ElementTree(ET.fromstring(response.content))\n",
    "root = tree.getroot()\n",
    "\n",
    "# Extract all <loc> URLs and clean them\n",
    "# The namespace might need to be handled if present\n",
    "namespace = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
    "urls = [url.text.strip().replace('\"', '') for url in root.findall('.//ns:loc', namespace)]\n",
    "\n",
    "# Save cleaned URLs to a CSV file\n",
    "csv_file = 'sitemap_urls.csv'\n",
    "with open(csv_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['URL'])  # Write header row\n",
    "    for url in urls:\n",
    "        writer.writerow([url])\n",
    "\n",
    "print(f\"Saved {len(urls)} cleaned URLs to {csv_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from selenium import webdriver \n",
    "from browsermobproxy import Server\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from chromedriver_py import binary_path \n",
    "# Import statements as before\n",
    "\n",
    "# Function to wait for an element to be present\n",
    "def wait_for_element_presence(driver, by, selector, timeout=10):\n",
    "    return WebDriverWait(driver, timeout).until(\n",
    "        EC.presence_of_element_located((by, selector))\n",
    "    )\n",
    "\n",
    "# Main Function \n",
    "if __name__ == \"__main__\":\n",
    "    path_to_browsermobproxy = \"/home/wlodzimierrr/tools/browsermob-proxy-2.1.4/bin/\"\n",
    "    server = Server(path_to_browsermobproxy + \"browsermob-proxy\", options={'port': 8090})\n",
    "    server.start()\n",
    "\n",
    "    proxy = server.create_proxy(params={\"trustAllServers\": \"true\"})\n",
    "\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('headless')\n",
    "    options.add_argument(\"--ignore-certificate-errors\")\n",
    "    options.add_argument(\"--proxy-server={0}\".format(proxy.proxy))\n",
    "\n",
    "    service = Service(executable_path=binary_path)\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    try:\n",
    "        # Initialize an empty list to store HAR entries\n",
    "        network_logs = []\n",
    "        \n",
    "        # Read URLs from CSV file\n",
    "        with open('sitemap_urls.csv', 'r', newline='', encoding='utf-8') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            urls = [row[0] for row in reader]\n",
    "        \n",
    "        # Iterate over each URL from the sitemap\n",
    "        for url in urls:\n",
    "            time.sleep(10)\n",
    "            print(f\"Navigating to: {url}\")\n",
    "            \n",
    "            # Start capturing HAR data for the current URL\n",
    "            proxy.new_har(url)\n",
    "            \n",
    "            # Load the URL in the browser\n",
    "            driver.get(url)\n",
    "            \n",
    "            # Check if the \"Load More\" button exists\n",
    "            try:\n",
    "                load_more_element = wait_for_element_presence(driver, By.CSS_SELECTOR, '[data-test-id=\"load-more-products\"]')\n",
    "                \n",
    "                # Scroll into view if necessary\n",
    "                actions = ActionChains(driver)\n",
    "                actions.move_to_element(load_more_element).perform()\n",
    "                time.sleep(1)\n",
    "                \n",
    "                # Click the \"Load More\" button using JavaScript to bypass interception\n",
    "                driver.execute_script(\"arguments[0].click();\", load_more_element)\n",
    "                \n",
    "                # Wait for additional content to load\n",
    "                time.sleep(5)\n",
    "            \n",
    "            except TimeoutException:\n",
    "                # Handle the case where \"Load More\" button doesn't exist\n",
    "                print(\"No 'Load More' button found. Proceeding with the current page content.\")\n",
    "            \n",
    "            # Capture the HAR data after the page has fully loaded\n",
    "            har_entry = proxy.har\n",
    "            \n",
    "            # Append the HAR entry to the list\n",
    "            network_logs.append(har_entry)\n",
    "    \n",
    "        # Write the list of HAR entries to a file\n",
    "        with open(\"network_logs_all.har\", \"a\", encoding=\"utf-8\") as f:\n",
    "            json.dump(network_logs, f, ensure_ascii=False)\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        server.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "import csv\n",
    "\n",
    "# Path to your HAR file\n",
    "har_file_path = \"network_logs_all.har\"\n",
    "csv_file = 'all_tradepoint_category_codes.csv'\n",
    "\n",
    "all_codes = []\n",
    "\n",
    "try:\n",
    "    # Read the HAR File and parse it using JSON\n",
    "    with open(har_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        logs = json.load(f)\n",
    "        \n",
    "    # Open the CSV file once and write the header row\n",
    "    with open(csv_file, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['category_code'])  # Write header row\n",
    "\n",
    "    # Iterate through the logs and extract the desired URLs and parameters\n",
    "    for log in logs:\n",
    "        network_logs = log['log']['entries']\n",
    "        for entry in network_logs:\n",
    "            try:\n",
    "                url = entry['request']['url']\n",
    "                parsed_url = urlparse(url)\n",
    "                query_params = parse_qs(parsed_url.query)\n",
    "                \n",
    "                # Find the entry with 'filter[category]' parameter\n",
    "                if 'filter[category]' in query_params:\n",
    "                    value_param = query_params['filter[category]'][0]\n",
    "                    print(f\"URL: {url}\")\n",
    "                    print(f\"Value of 'filter[category]': {value_param}\")\n",
    "                    all_codes.append(value_param)\n",
    "                        \n",
    "            except KeyError:\n",
    "                pass  # If 'request' or 'url' key is missing, skip this entry\n",
    "\n",
    "                        \n",
    "    # Append the value to the CSV file\n",
    "    with open(csv_file, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        all_codes = set(all_codes)\n",
    "        for code in all_codes:\n",
    "                writer.writerow([code])\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: HAR file '{har_file_path}' not found.\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error parsing JSON: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'Accept': 'application/json, text/plain, */*',\n",
    "    'Accept-Language': 'en-GB,en;q=0.9,en-US;q=0.8,pl;q=0.7,nl;q=0.6',\n",
    "    'Authorization': 'Atmosphere atmosphere_app_id=kingfisher-LTbGHXKinHaJSV86nEjf0KnO70UOVE6UcYAswLuC',\n",
    "    'Cache-Control': 'no-cache',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Origin': 'https://www.trade-point.co.uk',\n",
    "    'Pragma': 'no-cache',\n",
    "    'Referer': 'https://www.trade-point.co.uk/',\n",
    "    'Sec-Fetch-Dest': 'empty',\n",
    "    'Sec-Fetch-Mode': 'cors',\n",
    "    'Sec-Fetch-Site': 'cross-site',\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36',\n",
    "    'X-Context-Location': '/kitchen/kitchen-cabinets/kitchen-doors.cat',\n",
    "    'sec-ch-ua': '\"Google Chrome\";v=\"125\", \"Chromium\";v=\"125\", \"Not.A/Brand\";v=\"24\"',\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'sec-ch-ua-platform': '\"Windows\"',\n",
    "    'x-dtc': 'sn=\"v_4_srv_-2D20_sn_3DVN1BO5L1CBBVKHKCI1EI1OPMMJFPCD\", pc=\"-20$218348505_48h39vCMRFHLTDBVQCPNAPAHBUKCANTLTWOTFH-0e0\", v=\"17112790819981S31G3BNLBCJ3II45KG52PONU7NP0H1A\", app=\"d09643a9157f0c88\", r=\"https://www.trade-point.co.uk/kitchen/kitchen-cabinets/kitchen-doors.cat?Brand=GoodHome&Range=Stevia\"',\n",
    "    'x-tenant': 'TPUK',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import requests\n",
    "import csv\n",
    "import json\n",
    "\n",
    "def initial_request(category_code):\n",
    "    \"\"\"Make initial API request.\"\"\"\n",
    "    try:\n",
    "        time.sleep(random.uniform(5, 10))\n",
    "        response = requests.get(\n",
    "            f'https://api.kingfisher.com/v2/mobile/products/TPUK?filter[category]={category_code}&include=content&page[size]=1',\n",
    "            headers=headers,\n",
    "        )\n",
    "        response.raise_for_status()  \n",
    "        return response.json()\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"HTTP error occurred: {http_err}\")\n",
    "    except requests.exceptions.ConnectionError as conn_err:\n",
    "        print(f\"Connection error occurred: {conn_err}\")\n",
    "    except requests.exceptions.Timeout as timeout_err:\n",
    "        print(f\"The request timed out: {timeout_err}\")\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        print(f\"An error occurred while handling your request: {err}\")\n",
    "    return None  \n",
    "\n",
    "def get_total_page_count(response):\n",
    "    \"\"\"Extract total page count from API response.\"\"\"\n",
    "    if response and response.status_code == 200:\n",
    "        try:\n",
    "            data = response.json()\n",
    "            meta = data.get('meta', {})\n",
    "            paging = meta.get('paging', None)\n",
    "            if paging is None:\n",
    "                print(\"Paging data is missing in the response.\")\n",
    "                return 0, False, data\n",
    "            total_results = paging.get('totalResults', 0)\n",
    "            return total_results, True, data\n",
    "        except json.JSONDecodeError as e:\n",
    "           print(f\"JSON decode error: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "    else:\n",
    "        print(\"Failed to fetch data or no response\")\n",
    "        return 0, False, None\n",
    "\n",
    "def process_category_codes(input_csv, output_csv):\n",
    "    \"\"\"Process category codes from input CSV and save relevant data to output CSV.\"\"\"\n",
    "    with open(input_csv, 'r', newline='', encoding='utf-8') as csvfile: \n",
    "        reader = csv.reader(csvfile)\n",
    "        next(reader, None)\n",
    "        for code in reader:\n",
    "            category_code = code[0]\n",
    "            print(f\"checking: {category_code}\")\n",
    "            response = initial_request(category_code)\n",
    "            count, is_valid, data = get_total_page_count(response)\n",
    "            if is_valid and data:\n",
    "                index = len(data['data'][0]['attributes']['breadcrumbList']) - 1 \n",
    "                page_url = data['data'][0]['attributes']['pdpURL']\n",
    "                category = data['data'][0]['attributes']['breadcrumbList'][index - 1]['name']\n",
    "                subcategory = data['data'][0]['attributes']['breadcrumbList'][index - 2]['name']\n",
    "                print(f\"count: {count} category code: {page_url}\")\n",
    "                if count > 1:         \n",
    "                    with open(output_csv, mode='a', newline='') as file:\n",
    "                        writer = csv.writer(file)\n",
    "                        writer.writerow([page_url, category, subcategory, category_code])\n",
    "                        print(f\"saved: {category_code}\")\n",
    "\n",
    "# Define the input and output CSV file paths\n",
    "input_csv = 'all_tradepoint_category_codes.csv'\n",
    "output_csv = 'sorted_tradepoint_codes.csv'\n",
    "\n",
    "# # Process the category codes\n",
    "# process_category_codes(input_csv, output_csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrapers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
